{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Neural Data Science_\n",
    "\n",
    "Lecturer: Dr. Jan Lause, Prof. Dr. Philipp Berens\n",
    "\n",
    "Tutors: Jonas Beck, Fabio Seel, Julius Würzler\n",
    "\n",
    "Summer term 2025\n",
    "\n",
    "Student names: Nina Lutz, Mathis Nommensen\n",
    "\n",
    "LLM Disclaimer: We used ChatGPT for trouble shooting and ...?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Lab 6\n",
    "\n",
    "In this exercise we are going to fit a latent variable model (Poisson GPFA) to both toy data and real data from monkey primary visual cortex. For details, see [Ecker et al. 2014](https://www.cell.com/neuron/pdf/S0896-6273(14)00104-4.pdf)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### 1. Code \n",
    "\n",
    "The toolbox we are going to use contains an implementation of the EM algorithm to fit the poisson-gpfa. \n",
    "\n",
    "Assuming you `git clone https://github.com/berenslab/poisson-gpfa` to the parent directory and have the following directory structure:\n",
    "\n",
    "\n",
    "```\n",
    "├── data/\n",
    "│   └── nds_cl_6_data.mat\n",
    "├── poisson-gpfa/\n",
    "├── notebooks\n",
    "│   └── CodingLab6.ipynb\n",
    "├── matplotlib_style.txt\n",
    "├── requirements.txt\n",
    "```\n",
    "\n",
    "then you can import the related functions via:\n",
    "\n",
    "```\n",
    "import sys\n",
    "sys.path.append('../poisson-gpfa/')\n",
    "sys.path.append('../poisson-gpfa/funs')\n",
    "\n",
    "import funs.util as util\n",
    "import funs.engine as engine\n",
    "```\n",
    "\n",
    "Change the paths if you have a different directory structure. For the details of the algorithm, please refer to the thesis `hooram_thesis.pdf` from ILIAS.\n",
    "\n",
    "### 2. Data\n",
    "\n",
    "Download the data file ```nds_cl_6_data.mat``` from ILIAS and save it in a ```data/``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# style\n",
    "import seaborn as sns\n",
    "\n",
    "# poisson-gpfa\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../poisson-gpfa/\")\n",
    "sys.path.append(\"../poisson-gpfa/funs\")\n",
    "\n",
    "\n",
    "import funs.util as util\n",
    "import funs.engine as engine\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext jupyter_black\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark --time --date --timezone --updated --python --iversions --watermark -p sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"../matplotlib_style.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Generate some toy data to test the poisson-GPFA code\n",
    "\n",
    "We start by verifying our code on toy data. The cell below contains code to generate data for 30 neurons, 100 trials (1000 ms each) and 50ms bin size. The neurons' firing rate $\\lambda_k$ is assumed to be a constant $d_k$ modulated by a one-dimensional latent state $x$, which is drawn from a Gaussian process:\n",
    "\n",
    "$\\lambda_k = \\exp(c_kx + d_k)$\n",
    "\n",
    "Each neuron's weight $c_k$ is drawn randomly from a normal distribution and spike counts are sampled form a Poisson distribution with rate $\\lambda_k$.\n",
    "\n",
    "Your task is to fit a Poisson GPFA model with one latent variable to this data (see `engine.PPGPFAfit`).\n",
    "\n",
    "Hint: You can use `util.dataset?`, `engine.PPGPFAfit?` or `util.initializeParams?` to find out more about the provided package.\n",
    "\n",
    "*Grading: 3 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# simulate a training set (0.5 pts)\n",
    "# ---------------------------------\n",
    "\n",
    "# Initialize random number generator\n",
    "np.random.seed(123)\n",
    "\n",
    "# Specify dataset & fitting parameters\n",
    "xdim = 1\n",
    "ydim = 30\n",
    "numTrials = 100\n",
    "trialDur = 1000  # in ms\n",
    "binSize = 50  # in ms\n",
    "maxEMiter = 100\n",
    "dOffset = 1  # controls firing rate\n",
    "\n",
    "# Sample from the model (make a toy dataset)\n",
    "training_set = util.dataset(\n",
    "    seed=np.random.randint(10000),\n",
    "    xdim=xdim,\n",
    "    ydim=ydim,\n",
    "    numTrials=numTrials,\n",
    "    trialDur=trialDur,\n",
    "    binSize=binSize,\n",
    "    dOffset=dOffset,\n",
    "    fixTau=True,\n",
    "    fixedTau=np.linspace(0.1, 0.5, xdim),\n",
    "    drawSameX=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# fit the model (0.5 pts)\n",
    "# -----------------------\n",
    "\n",
    "# Initialize parameters using Poisson-PCA\n",
    "initParams = util.initializeParams(xdim, ydim, training_set)\n",
    "\n",
    "# choose sensible parameters and run fit\n",
    "fitToy = engine.PPGPFAfit(\n",
    "    experiment=training_set,\n",
    "    initParams=initParams,\n",
    "    inferenceMethod=\"laplace\",\n",
    "    EMmode=\"Batch\",  # using vanilla (batch) EM. for online EM use \"Online\"\n",
    "    maxEMiter=maxEMiter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful functions\n",
    "def allTrialsState(fit: engine.PPGPFAfit, p) -> np.ndarray:\n",
    "    \"\"\"Reshape the latent signal and the spike counts\"\"\"\n",
    "    x = np.zeros([p, 0])\n",
    "    for i in range(len(fit.infRes[\"post_mean\"])):\n",
    "        x = np.concatenate((x, fit.infRes[\"post_mean\"][i]), axis=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def allTrialsX(training_set: util.dataset) -> np.ndarray:\n",
    "    \"\"\"Reshape the ground truth\n",
    "    latent signal and the spike counts\"\"\"\n",
    "    x_gt = np.array([])\n",
    "    for i in range(len(training_set.data)):\n",
    "        x_gt = np.concatenate((x_gt, training_set.data[i][\"X\"][0]), axis=0)\n",
    "    return x_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ground truth vs. inferred model\n",
    "Verify your fit by plotting both ground truth and inferred parameters for:\n",
    "1. weights C\n",
    "2. biases d\n",
    "3. latent state x \n",
    "\n",
    "Note that the sign of fitted latent state and its weights are ambiguous (you can flip both without changing the model). Make sure you correct the sign for the plot if it does not match the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All trials latent state vector\n",
    "x_est = allTrialsState(fitToy, 1)\n",
    "x_true = allTrialsX(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# Plot ground truth vs. inferred model\n",
    "# Plot the weights `C`, biases `d` and latent states (2 pts)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\n",
    "# add plot\n",
    "fig, ax = plt.subplot_mosaic([[\"C\", \"d\"], [\"latent\", \"latent\"]])\n",
    "# For d & C consider also plotting the optimal weights as a dotted line for reference\n",
    "\n",
    "# estimated params\n",
    "C_est = fitToy.optimParams[\"C\"]\n",
    "d_est = fitToy.optimParams[\"d\"]\n",
    "\n",
    "# ground truth\n",
    "C_true = training_set.params[\"C\"]\n",
    "d_true = training_set.params[\"d\"]\n",
    "\n",
    "\n",
    "ax[\"C\"].plot(-C_est, label=\"Estimated\")  # flipped sign for better comparison\n",
    "ax[\"C\"].plot(C_true, linestyle=\"--\", label=\"True\")\n",
    "ax[\"C\"].set_title(\"Estimated vs. True Weights\")\n",
    "\n",
    "ax[\"d\"].plot(d_est)\n",
    "ax[\"d\"].plot(d_true, linestyle=\"--\")\n",
    "ax[\"d\"].set_title(\"Estimated vs. True Biases\")\n",
    "\n",
    "# For the latent states consider seperating each trial by a vertical line\n",
    "# plot only for a subset of trials\n",
    "T = training_set.data[0][\"Y\"].shape[1]  # time bins per trial\n",
    "x_est_subset = x_est[0][0 : 5 * T]\n",
    "x_true_subset = x_true[0 : 5 * T]\n",
    "\n",
    "ax[\"latent\"].plot(-x_est_subset)  # flipped sign for better comparison\n",
    "ax[\"latent\"].plot(x_true_subset, linestyle=\"--\")\n",
    "ax[\"latent\"].set_title(\"Estimated vs. True Latent State\")\n",
    "ax[\"latent\"].legend(\n",
    "    handles=[\n",
    "        plt.Line2D([], [], color=\"tab:orange\", linestyle=\"--\", label=\"True\"),\n",
    "        plt.Line2D([], [], color=\"tab:blue\", label=\"Estimated\"),\n",
    "    ],\n",
    "    loc=\"lower left\",\n",
    "    fontsize=9,\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Fit GPFA model to real data. \n",
    "\n",
    "We now fit the model to real data and cross-validate over the dimensionality of the latent variable.\n",
    "\n",
    "*Grading: 4 pts*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "The cell below implements loading the data and encapsulates it into a class that matches the interface of the Poisson GPFA engine. You don't need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EckerDataset:\n",
    "    \"\"\"Loosy class\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        subject_id: int = 0,\n",
    "        ydim: int = 55,\n",
    "        trialDur: int = 2000,\n",
    "        binSize: int = 100,\n",
    "        numTrials: int = 100,\n",
    "        ydimData: bool = False,\n",
    "        numTrData: bool = True,\n",
    "    ):\n",
    "        # T = binSize #int(trialDur/binSize)\n",
    "        T = int(trialDur / binSize)\n",
    "        matdat = sio.loadmat(path)\n",
    "        self.matdat = matdat\n",
    "        data = []\n",
    "        trial_durs = []\n",
    "        for trial_id in range(numTrials):\n",
    "            trial_time = matdat[\"spikeTimes\"][:, trial_id][0]\n",
    "            trial_big_time = np.min(trial_time)\n",
    "            trial_end_time = np.max(trial_time)\n",
    "            trial_durs.append(trial_end_time - trial_big_time)\n",
    "        for trial_id in range(numTrials):\n",
    "            Y = []\n",
    "            spike_time = []\n",
    "            data.append(\n",
    "                {\n",
    "                    \"Y\": matdat[\"spikeCounts\"][:, :, trial_id],\n",
    "                    \"spike_time\": matdat[\"spikeTimes\"][:, trial_id],\n",
    "                }\n",
    "            )\n",
    "        self.T = T\n",
    "        self.trial_durs = trial_durs\n",
    "        self.data = data\n",
    "        self.trialDur = trialDur\n",
    "        self.binSize = binSize\n",
    "        self.numTrials = numTrials\n",
    "        self.ydim = ydim\n",
    "        util.dataset.getMeanAndVariance(self)\n",
    "        util.dataset.getAvgFiringRate(self)\n",
    "        util.dataset.getAllRaster(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/nds_cl_6_data.mat\"\n",
    "data = EckerDataset(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Poisson GPFA models and perform model comparison\n",
    "\n",
    "Split the data into 80 trials used for training and 20 trials held out for performing model comparison. On the training set, fit models using one to five latent variables. Compute the performance of each model on the held-out test set.\n",
    "\n",
    "Hint: You can use the `crossValidation` function in the Poisson GPFA package.\n",
    "\n",
    "Optional: The `crossValidation` function computes the sum of the squared errors (SSE) on the test set, which is not ideal. The predictive log-likelihood under the Poisson model would be a better measure, which you are welcome to compute instead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation for log-likelihood\n",
    "\n",
    "_You can add your calculations in_ $\\LaTeX$ _here_.\n",
    "\n",
    "$\\lambda_k(x_t) = \\exp(C x_t + d) = \\exp(c_k x_t + d_k)$; $y_t \\sim Poisson(\\lambda_t)$; $p_\\lambda(y) = \\frac{\\lambda^y\\exp(-\\lambda)}{y!}$\n",
    "\n",
    "$p_\\lambda(x_t) = p_\\lambda(y_k \\mid x_t) = \\prod_{k=1}^K \\frac{\\exp\\left( c_k x_t + d_k \\right)^{y_{kt}} \\cdot \\exp\\left( -\\exp(c_k x_t + d_k) \\right)}{y_{kt}!}$, whereas K = number of neurons\n",
    "\n",
    "\n",
    "$L(\\lambda_k; x_1, ..., x_N) = \\prod_{t=1}^T p_\\lambda(y_k \\mid x_t)= \\prod_{t=1}^T \\prod_{k=1}^K \\frac{\\exp(c_k x_t + d_k)^{y_{kt}} \\cdot e^{-\\exp(c_k x_t + d_k)}}{y_{kt}!}$, whereas T = number of time bins per trial\n",
    "\n",
    "$log(L) = l(\\lambda_k; x_1, ..., x_N) = \\sum_{t=1}^T \\sum_{k=1}^K \\log\\left(\\frac{\\left[\\exp(c_k x_t + d_k)\\right]^{y_{kt}} \\cdot e^{-\\exp(c_k x_t + d_k)}}{y_{kt}!}\\right) \\\\ = \\sum_{t=1}^T \\sum_{k=1}^K\\left[y_{kt}(c_k x_t + d_k)- \\exp(c_k x_t + d_k)- \\log(y_{kt}!)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Perfom cross validation (1 pt)\n",
    "# ------------------------------\n",
    "\n",
    "### SSE version (optional version could be done later)\n",
    "\n",
    "# do the actual cross validation\n",
    "# split: 80 training / 20 validation\n",
    "numTrainingTrials = 80\n",
    "numTestTrials = 20\n",
    "latent_vars = 5\n",
    "\n",
    "maxEMiter = 50  # number of EM iterations for each fold\n",
    "\n",
    "xval = util.crossValidation(\n",
    "    experiment=data,\n",
    "    numTrainingTrials=numTrainingTrials,\n",
    "    numTestTrials=numTestTrials,\n",
    "    maxXdim=latent_vars,\n",
    "    maxEMiter=maxEMiter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the test error\n",
    "\n",
    "Make a plot of the test error for the five different models. As a baseline, please also include the test error of a model without a latent variable. This is essentially the SSE of a constant rate model (or Poisson likelihood if you did the optional part above). Note: We assume a constant firing rate across trials, but not necessarily across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Compute and plot the test errors for the different latent variable models (1 pt)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "train_set, test_set = util.splitTrainingTestDataset(\n",
    "    data, numTrainingTrials=80, numTestTrials=20\n",
    ")\n",
    "# Estimate mean firing rate per neuron per time bin from training set\n",
    "mean_rate = np.zeros_like(test_set.data[0][\"Y\"], dtype=np.float64)\n",
    "for trial in train_set.data:\n",
    "    mean_rate += trial[\"Y\"]\n",
    "mean_rate /= len(train_set.data)\n",
    "\n",
    "# Compute SSE over test set\n",
    "baseline_error = 0\n",
    "for trial in test_set.data:\n",
    "    baseline_error += np.sum((trial[\"Y\"] - mean_rate) ** 2)\n",
    "\n",
    "# model errors\n",
    "model_errors = xval.errs\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot model errors (dims 1 to 5)\n",
    "plt.plot(\n",
    "    range(1, len(model_errors) + 1),\n",
    "    model_errors,\n",
    "    marker=\"o\",\n",
    "    label=\"Latent Variable Models\",\n",
    ")\n",
    "\n",
    "# Plot baseline (horizontal line)\n",
    "plt.hlines(\n",
    "    baseline_error,\n",
    "    1,\n",
    "    len(model_errors),\n",
    "    colors=\"red\",\n",
    "    linestyles=\"dashed\",\n",
    "    label=\"Baseline (No Latent Var)\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Latent Dimensionality\")\n",
    "plt.ylabel(\"Test Error (SSE)\")\n",
    "plt.title(\"Model Performance on Held-Out Test Set\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Compute and plot the test errors for the different latent variable models\n",
    "# and answer the questions below (1+1 pts)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Your plot here\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "# plot model error\n",
    "ax.plot(\n",
    "    range(1, len(model_errors) + 1),\n",
    "    model_errors,\n",
    "    marker=\"o\",\n",
    "    label=\"Latent Variable Models\",\n",
    ")\n",
    "# plot baseline\n",
    "ax.axhline(\n",
    "    baseline_error, linestyle=\"--\", color=\"red\", label=\"Baseline (No Latent Var)\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Latent Dimensionality\")\n",
    "ax.set_ylabel(\"Test Error (SSE)\")\n",
    "ax.set_title(\"Test Error vs. Latent Dimensionality\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Is the baseline error (0 latents) about what you expect in relation to the other models? Why or why not?\n",
    "\n",
    "* Yes, the baseline error is what we expect. It is higher than the errors from models with 1 to 5 latent variables, indicating that the latent variable models capture meaningful shared structure in the data but the constant-rate baseline cannot. The baseline assumes each neuron has a fixed firing rate (true, across trials), while latent variable models explain coordinated fluctuations across neurons, improving predictive performance on held-out data.\n",
    "\n",
    "Can you identify a \"best model\". If so, which is it and what does this say about the structure of the latent state?\n",
    "\n",
    "* The model with 4 latent variables looks like the \"best model\". It achieves a low test error and shows weaker improvement after that point. This suggests that the underlying structure of the neural data is low-dimensional and can be effectively captured with just four latent factors. These factors likely reflect shared variability among neurons due to common inputs, behavioral states, or network dynamics. Adding more dimensions provides only minimal gain, indicating that most of the meaningful structure is already explained by the first four latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Visualization: population rasters and latent state.\n",
    "Use the model with a single latent state. \n",
    "\n",
    "Create a raster plot where you show for each trial the spikes of all neurons as well as the trajectory of the latent state `x` (take care of the correct time axis). Sort the neurons by their weights `c_k`. Plot only the first 20 trials.\n",
    "\n",
    "*Grading: 2 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import matlib\n",
    "\n",
    "# get one fitted model from xdim 1 to 5\n",
    "fit = xval.fits[0]  # for xdim=1\n",
    "C = fit.optimParams[\"C\"][:, 0]  # weights c_k for xdim=1, shape: (neurons, 1)\n",
    "\n",
    "# sort neurons by their weights (increasing order)\n",
    "neurons_sorted = np.argsort(C)\n",
    "\n",
    "# Your plot here\n",
    "fig, axs = plt.subplots(10, 2, figsize=(14, 14))\n",
    "\n",
    "ts = np.linspace(50, 2000, 100)\n",
    "xa = 0.15\n",
    "xs = 0.7 * xa * np.sin(ts / 1000 * 3.4 * 2 * np.pi) + xa\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Raster Plot of Spikes Sorted by Neuron Weight ($c_k$) with Latent State Overlay\",\n",
    "    fontsize=16,\n",
    "    y=1.02,  # adjusts vertical position if needed\n",
    ")\n",
    "\n",
    "with sns.axes_style(\"ticks\"):\n",
    "    for ntrial, ax in enumerate(axs.flat):\n",
    "        x = range(0, 2000, 100)  # assume binsize of 100ms\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # plot the raster for each neuron and latent state (2 pts)\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        # hint: can be plotted on top of the corresponding raster\n",
    "\n",
    "        # sort neurons by weight (from lowest to highest)\n",
    "        # sorted spike matrix per trial\n",
    "        Y = data.data[ntrial][\"Y\"]\n",
    "        Y_sorted = Y[neurons_sorted, :]\n",
    "\n",
    "        # raster\n",
    "        for n, neuron in enumerate(Y_sorted):\n",
    "            spike_bins = np.where(neuron > 0)[0]\n",
    "            for t_bin in spike_bins:\n",
    "                ax.vlines(\n",
    "                    x=t_bin * data.binSize,\n",
    "                    ymin=n,\n",
    "                    ymax=n + 0.8,\n",
    "                    color=\"gray\",\n",
    "                    linewidth=0.5,\n",
    "                )\n",
    "\n",
    "        # plot the latent state trajectory\n",
    "        x_latent = fit.infRes[\"post_mean\"][ntrial][0]  # shape: (time bins,)\n",
    "        x_scaled = (x_latent - np.mean(x_latent)) / np.std(x_latent)\n",
    "        x_scaled = (x_scaled * 0.7 * xa + xa) * data.ydim\n",
    "        ax.plot(x, x_scaled, color=\"tab:blue\", linewidth=1.5, label=\"Latent State\")\n",
    "\n",
    "        if ntrial == 0:\n",
    "            ax.legend()\n",
    "            # Label for spike marks\n",
    "            ax.plot([200, 200], [0, 0.8], color=\"gray\", linewidth=1)\n",
    "            ax.text(210, 0.2, \"Neuron spike\", color=\"gray\", fontsize=6)\n",
    "\n",
    "        if ntrial == 1:\n",
    "            ax.plot([1000, 2000], [-30, -30], color=\"green\")\n",
    "            ax.text(1300, -50, \"1sec\")\n",
    "        if ntrial < 2:\n",
    "            ax.plot(ts, (xs * 40) + data.ydim, \"k\", color=\"black\")\n",
    "\n",
    "        # Set y-axis label only on leftmost column (even-numbered trials)\n",
    "        if ntrial % 2 == 0:\n",
    "            ax.set_ylabel(\"Neurons\")\n",
    "\n",
    "        # Set x-axis label only on bottom row (last two subplots)\n",
    "        if ntrial >= 18:\n",
    "            ax.set_xlabel(\"Time (ms)\")\n",
    "\n",
    "        # ax.set_yticks([])\n",
    "        # ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Visualization of covariance matrix.\n",
    "\n",
    "Plot (a) the covariance matrix of the observed data as well as its approximation using (b) one and (c) five latent variable(s). Use the analytical solution for the covariance matrix of the approximation*. Note that the solution is essentially the mean and covariance of the [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution).\n",
    "\n",
    "$ \\mu = \\exp(\\frac{1}{2} \\text{ diag}(CC^T)+d)$\n",
    "\n",
    "$ \\text{Cov}= \\mu\\otimes\\mu^T \\odot \\exp(CC^T)+\\mu\\cdot \\mathbb{I} - \\mu\\otimes\\mu^T$ \n",
    "\n",
    "*[Krumin, M., and Shoham, S. (2009). Generation of Spike Trains with Controlled Auto- and Cross-Correlation Functions. Neural Computation 21, 1642–1664](http://www.mitpressjournals.org/doi/10.1162/neco.2009.08-08-847).\n",
    "\n",
    "*Grading: 3 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Complete the analytical solution for the covariance matrix of\n",
    "# the approximation using the provide equations (2 pts)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "def cov(fit: engine.PPGPFAfit) -> np.ndarray:\n",
    "\n",
    "    C = fit.optimParams[\"C\"]\n",
    "    d = fit.optimParams[\"d\"]\n",
    "\n",
    "    CCt = C @ C.T\n",
    "    diag_CCt = np.sum(CCt, axis=1)  # diag(CC^T)\n",
    "    mu = np.exp(0.5 * diag_CCt + d)\n",
    "\n",
    "    outer_mu = np.outer(mu, mu)\n",
    "    c = outer_mu * np.exp(CCt) + np.diag(mu) - outer_mu\n",
    "\n",
    "    return c, mu\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Plot the covariance matrix of\n",
    "# (1) the observed data\n",
    "# (2) its approximation using 1 latent variable\n",
    "# (3) its approximation using 5 latent variable\n",
    "# and explain how they compare (1+1 pts).\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "obs_corr = np.cov(data.all_raster)\n",
    "opt_r1, mu1 = cov(xval.fits[0])\n",
    "opt_r5, mu5 = cov(xval.fits[4])\n",
    "\n",
    "# HINT: Think about which type of colormap and ranges are appropriate here.\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3.5))\n",
    "# add plot to visualize the differences in the covariance matrices\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# option1 : take log to scale down plots\n",
    "# => clip values beforehand to avoid zero or negative values for LogNorm\n",
    "epsilon = 1e-6\n",
    "obs_corr_safe = np.clip(obs_corr, epsilon, None)\n",
    "opt_r1_safe = np.clip(opt_r1, epsilon, None)\n",
    "opt_r5_safe = np.clip(opt_r5, epsilon, None)\n",
    "\n",
    "# observed covariance\n",
    "im0 = axs[0].imshow(obs_corr_safe, cmap=\"viridis\", norm=LogNorm())\n",
    "axs[0].set_title(\"Observed Covariance\")\n",
    "axs[0].set_xlabel(\"Neuron\")\n",
    "axs[0].set_ylabel(\"Neuron\")\n",
    "\n",
    "# 1 latent variable\n",
    "im1 = axs[1].imshow(opt_r1_safe, cmap=\"viridis\", norm=LogNorm())\n",
    "axs[1].set_title(\"Approximation (xdim=1)\")\n",
    "axs[1].set_xlabel(\"Neuron\")\n",
    "\n",
    "# 5 latent variables\n",
    "im2 = axs[2].imshow(opt_r5_safe, cmap=\"viridis\", norm=LogNorm())\n",
    "axs[2].set_title(\"Approximation (xdim=5)\")\n",
    "axs[2].set_xlabel(\"Neuron\")\n",
    "\n",
    "fig.colorbar(im2, ax=axs, orientation=\"vertical\", fraction=0.025, pad=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "What do you see / expect to see?\n",
    "\n",
    "_YOUR ANSWER GOES HERE_\n",
    "\n",
    "As expected, there are large values on the diagonal, showing how each neuron varies. Without a scaling with LogNorm, only diagonal values were visible. To get a better look at the overall structure, we scaled the values down, which resulted in plots with more detail, mostly for the off-diagonal values. As expected, 5 latent variables approximate the original data much better, especially in those off-diagonal values. 1 latent variable only models the firing rates as being influenced by 1 factor, so the matrix is only a low-rank approximation of the true covariance. Off-diagonal structure is partly visible, but details are missing. 5 latent variables are still visibly different, but much closer to the true covariance, with more detail in the off-diagonal structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
